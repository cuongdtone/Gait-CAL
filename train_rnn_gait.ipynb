{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from datasets.ae_feat_dataset import FeatDataset\n",
    "from models.lstm import BiLSTM\n",
    "import torch\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "train_dataset = FeatDataset('dataset/train')\n",
    "val_dataset = FeatDataset('dataset/test')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "epochs = 200\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "BiLSTM(\n  (lstm): LSTM(324, 64, batch_first=True, bidirectional=True)\n  (linear): Linear(in_features=256, out_features=64, bias=True)\n  (relu): ReLU()\n  (dropout): Dropout(p=0.1, inplace=False)\n  (out): Linear(in_features=64, out_features=128, bias=True)\n)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "model = BiLSTM()\n",
    "model.to(device)\n",
    "model.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\n",
    "margin = 1.\n",
    "triplet_loss = torch.nn.TripletMarginWithDistanceLoss(distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[200] Training, 0.00%,0/15849, Loss=0.3824446201324463\n",
      "Epoch: [0]/[200] Training, 3.23%,512/15849, Loss=0.37649768590927124\n",
      "Epoch: [0]/[200] Training, 6.46%,1024/15849, Loss=0.3879859149456024\n",
      "Epoch: [0]/[200] Training, 9.69%,1536/15849, Loss=0.41673165559768677\n",
      "Epoch: [0]/[200] Training, 12.92%,2048/15849, Loss=0.3725932836532593\n",
      "Epoch: [0]/[200] Training, 16.15%,2560/15849, Loss=0.387528657913208\n",
      "Epoch: [0]/[200] Training, 19.38%,3072/15849, Loss=0.39086002111434937\n",
      "Epoch: [0]/[200] Training, 22.61%,3584/15849, Loss=0.38647735118865967\n",
      "Epoch: [0]/[200] Training, 25.84%,4096/15849, Loss=0.36221566796302795\n",
      "Epoch: [0]/[200] Training, 29.07%,4608/15849, Loss=0.401569664478302\n",
      "Epoch: [0]/[200] Training, 32.30%,5120/15849, Loss=0.39867106080055237\n",
      "Epoch: [0]/[200] Training, 35.54%,5632/15849, Loss=0.37596726417541504\n",
      "Epoch: [0]/[200] Training, 38.77%,6144/15849, Loss=0.39209672808647156\n",
      "Epoch: [0]/[200] Training, 42.00%,6656/15849, Loss=0.40474680066108704\n",
      "Epoch: [0]/[200] Training, 45.23%,7168/15849, Loss=0.37427088618278503\n",
      "Epoch: [0]/[200] Training, 48.46%,7680/15849, Loss=0.3954325318336487\n",
      "Epoch: [0]/[200] Training, 51.69%,8192/15849, Loss=0.4572732448577881\n",
      "Epoch: [0]/[200] Training, 54.92%,8704/15849, Loss=0.3839608132839203\n",
      "Epoch: [0]/[200] Training, 58.15%,9216/15849, Loss=0.3593619465827942\n",
      "Epoch: [0]/[200] Training, 61.38%,9728/15849, Loss=0.41042062640190125\n",
      "Epoch: [0]/[200] Training, 64.61%,10240/15849, Loss=0.39950016140937805\n",
      "Epoch: [0]/[200] Training, 67.84%,10752/15849, Loss=0.36910179257392883\n",
      "Epoch: [0]/[200] Training, 71.07%,11264/15849, Loss=0.412454754114151\n",
      "Epoch: [0]/[200] Training, 74.30%,11776/15849, Loss=0.36912715435028076\n",
      "Epoch: [0]/[200] Training, 77.53%,12288/15849, Loss=0.3869529068470001\n",
      "Epoch: [0]/[200] Training, 80.76%,12800/15849, Loss=0.3594033420085907\n",
      "Epoch: [0]/[200] Training, 83.99%,13312/15849, Loss=0.36747777462005615\n",
      "Epoch: [0]/[200] Training, 87.22%,13824/15849, Loss=0.39759618043899536\n",
      "Epoch: [0]/[200] Training, 90.45%,14336/15849, Loss=0.3920854926109314\n",
      "Epoch: [0]/[200] Training, 93.68%,14848/15849, Loss=0.39504092931747437\n",
      "Epoch: [0]/[200] Training, 96.91%,15360/15849, Loss=0.3883771598339081\n",
      "0.3888459378673184\n",
      "Epoch: [1]/[200] Training, 0.00%,0/15849, Loss=0.37466108798980713\n",
      "Epoch: [1]/[200] Training, 3.23%,512/15849, Loss=0.3959426283836365\n",
      "Epoch: [1]/[200] Training, 6.46%,1024/15849, Loss=0.41367724537849426\n",
      "Epoch: [1]/[200] Training, 9.69%,1536/15849, Loss=0.37764978408813477\n",
      "Epoch: [1]/[200] Training, 12.92%,2048/15849, Loss=0.3914966881275177\n",
      "Epoch: [1]/[200] Training, 16.15%,2560/15849, Loss=0.3605409860610962\n",
      "Epoch: [1]/[200] Training, 19.38%,3072/15849, Loss=0.37661561369895935\n",
      "Epoch: [1]/[200] Training, 22.61%,3584/15849, Loss=0.4145597815513611\n",
      "Epoch: [1]/[200] Training, 25.84%,4096/15849, Loss=0.4040203392505646\n",
      "Epoch: [1]/[200] Training, 29.07%,4608/15849, Loss=0.3521970510482788\n",
      "Epoch: [1]/[200] Training, 32.30%,5120/15849, Loss=0.39770957827568054\n",
      "Epoch: [1]/[200] Training, 35.54%,5632/15849, Loss=0.3664279282093048\n",
      "Epoch: [1]/[200] Training, 38.77%,6144/15849, Loss=0.39299848675727844\n",
      "Epoch: [1]/[200] Training, 42.00%,6656/15849, Loss=0.37286898493766785\n",
      "Epoch: [1]/[200] Training, 45.23%,7168/15849, Loss=0.3734503388404846\n",
      "Epoch: [1]/[200] Training, 48.46%,7680/15849, Loss=0.3552224934101105\n",
      "Epoch: [1]/[200] Training, 51.69%,8192/15849, Loss=0.3440983295440674\n",
      "Epoch: [1]/[200] Training, 54.92%,8704/15849, Loss=0.4261353015899658\n",
      "Epoch: [1]/[200] Training, 58.15%,9216/15849, Loss=0.43272486329078674\n",
      "Epoch: [1]/[200] Training, 61.38%,9728/15849, Loss=0.3616648316383362\n",
      "Epoch: [1]/[200] Training, 64.61%,10240/15849, Loss=0.37761780619621277\n",
      "Epoch: [1]/[200] Training, 67.84%,10752/15849, Loss=0.41061604022979736\n",
      "Epoch: [1]/[200] Training, 71.07%,11264/15849, Loss=0.4179116189479828\n",
      "Epoch: [1]/[200] Training, 74.30%,11776/15849, Loss=0.39062726497650146\n",
      "Epoch: [1]/[200] Training, 77.53%,12288/15849, Loss=0.38726806640625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    iter = 0\n",
    "    for batch_idx, (anchor, positive, negative) in enumerate(train_loader):\n",
    "\n",
    "        anchor = anchor.to(device)\n",
    "        positive = positive.to(device)\n",
    "        negative = negative.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        anchor_out = model(anchor.float())\n",
    "        positive_out = model(positive.float())\n",
    "        negative_out = model(negative.float())\n",
    "\n",
    "\n",
    "        output = triplet_loss(anchor_out, positive_out, negative_out)\n",
    "        output.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        print(f'Epoch: [%d]/[%d] Training, %.2f%%,{iter}/{len(train_dataset)}, Loss={output.item()}' % (epoch, epochs, iter*100/len(train_dataset)))\n",
    "        iter += len(anchor)\n",
    "        losses.append(output.item())\n",
    "    print(sum(losses)/len(losses))\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.93492377 -0.24233335\n",
      "0.9463702 -0.21094653\n",
      "0.98746496 -0.085279986\n",
      "0.8990053 -0.14777455\n",
      "0.974549 -0.16407453\n",
      "0.91302896 -0.28488055\n",
      "0.9742724 -0.123406775\n",
      "0.8666797 -0.054263745\n",
      "0.9800681 -0.21377677\n",
      "0.9667697 -0.23248772\n",
      "0.723328 -0.37068725\n",
      "0.90487546 0.9922157\n",
      "0.9006486 0.03421862\n",
      "0.97548574 0.9730354\n",
      "0.96246886 -0.11990808\n",
      "0.9172875 0.9979038\n",
      "0.98029006 -0.3672654\n",
      "0.943522 -0.27532157\n",
      "0.71167856 -0.17687374\n",
      "0.9295279 0.76083004\n",
      "0.9956552 -0.09590244\n",
      "0.9454024 -0.6619248\n",
      "0.98020005 -0.10497154\n",
      "0.9353707 -0.100796945\n",
      "0.93954754 0.90227723\n",
      "0.915203 0.984315\n",
      "0.9427173 0.93906873\n",
      "0.92574644 0.97098535\n",
      "0.970981 0.90607727\n",
      "0.93067354 -0.67093337\n",
      "0.9843961 -0.06573344\n",
      "0.9984557 -0.65910107\n",
      "0.99875265 0.32489637\n",
      "0.5831846 0.9005014\n",
      "0.9642543 0.2609868\n",
      "0.97541034 -0.17695755\n",
      "0.74179155 0.5975812\n",
      "0.9437297 -0.5062643\n",
      "0.983991 0.9244435\n",
      "0.8639407 -0.06669008\n",
      "0.97711146 -0.62418044\n",
      "0.99152243 0.9496718\n",
      "0.9909818 -0.6139446\n",
      "0.91971713 -0.180524\n",
      "0.9169249 -0.061829407\n",
      "0.69248176 0.882265\n",
      "0.6201116 0.8061057\n",
      "0.61670005 0.73836917\n",
      "0.8453418 0.8832881\n",
      "0.8134753 0.78041\n",
      "0.9834369 -0.4735815\n",
      "0.9742949 0.8756493\n",
      "0.99810594 -0.13974023\n",
      "0.9825348 0.6099387\n",
      "0.9549778 0.11117478\n",
      "0.95698696 -0.6657206\n",
      "0.97552824 0.7114505\n",
      "0.97111326 -0.5109215\n",
      "0.83548546 0.79506195\n",
      "0.9581674 0.8991928\n",
      "0.9670913 -0.15777007\n",
      "0.6971718 -0.14881912\n",
      "0.9945498 -0.53231657\n",
      "0.9382842 0.079256125\n",
      "0.9555814 0.9968446\n",
      "0.90893364 0.8704214\n",
      "0.88584346 -0.4930795\n",
      "0.8153244 0.019734262\n",
      "0.8656742 0.20385742\n",
      "0.8606474 -0.19566181\n",
      "0.9751369 0.9866494\n",
      "0.7994427 0.31159568\n",
      "0.96381676 -0.45075232\n",
      "0.78109586 0.26265135\n",
      "0.9721593 -0.22612783\n",
      "0.6192799 -0.0690124\n",
      "0.9883013 -0.5934153\n",
      "0.9767487 -0.007889759\n",
      "0.96949786 0.11051556\n",
      "0.9649743 0.37626666\n",
      "0.97273177 0.06691451\n",
      "0.4803536 -0.26678228\n",
      "0.9927413 0.9206774\n",
      "0.9787298 -0.3631044\n",
      "0.93398625 0.9391653\n",
      "0.9025397 0.41435665\n",
      "0.91701007 0.74558884\n",
      "0.49243447 0.7088393\n",
      "0.96785533 0.3761416\n",
      "0.7788866 0.6209478\n",
      "0.8598314 0.40379137\n",
      "0.98105276 -0.44816688\n",
      "0.9979996 -0.63254416\n",
      "0.9907825 -0.4206796\n",
      "0.9876049 0.9762199\n",
      "0.98706055 0.9934968\n",
      "0.9879524 0.1617516\n",
      "0.54585207 -0.537938\n",
      "0.66109675 0.84360397\n",
      "0.9548045 0.6405282\n",
      "0.9435449 0.74523646\n",
      "0.9712517 -0.06973219\n",
      "0.8200954 -0.07733788\n",
      "0.9817193 -0.7309419\n",
      "0.9909906 0.8732561\n",
      "0.99351984 -0.49623534\n",
      "0.994558 -0.6013602\n",
      "0.90147746 0.8940226\n",
      "0.9870953 0.39001843\n",
      "0.92610544 0.9539064\n",
      "0.94317865 0.036137532\n",
      "0.98599327 0.9169859\n",
      "0.92581177 0.04736437\n",
      "0.823994 -0.009480502\n",
      "0.92039293 -0.27983168\n",
      "0.8247561 0.8266122\n",
      "0.91791147 -0.8236293\n",
      "0.9431297 -0.2954445\n",
      "0.99400836 -0.45177042\n",
      "0.86900204 0.8616959\n",
      "0.92497 0.9648069\n",
      "0.94947344 0.5011761\n",
      "0.8908619 0.9143698\n",
      "0.92167515 0.85379255\n",
      "0.5425563 0.8205979\n",
      "0.92169446 0.93894106\n",
      "0.30008855 0.20673293\n",
      "0.7966528 0.6191553\n",
      "0.89643276 -0.2857249\n",
      "0.65526813 -0.0048469873\n",
      "0.53347284 0.70477736\n",
      "0.8484865 0.91766304\n",
      "0.92332375 -0.4010589\n",
      "0.9284261 -0.5288975\n",
      "0.99549484 -0.21060035\n",
      "0.6319799 -0.16626923\n",
      "0.6557426 -0.011562506\n",
      "0.99014175 -0.48772392\n",
      "0.95772904 0.35088003\n",
      "0.5419639 -0.5464202\n",
      "0.89774615 0.55031157\n",
      "0.48210794 -0.628246\n",
      "0.9282567 0.5004312\n",
      "0.94903564 0.54026324\n",
      "0.93640584 -0.09568442\n",
      "0.67702234 -0.05144714\n",
      "0.3589806 0.90095943\n",
      "0.7234373 0.6120042\n",
      "0.8725745 -0.7141027\n",
      "0.8808657 0.5852672\n",
      "0.92859507 0.74666935\n",
      "0.88430154 0.28585023\n",
      "0.3898626 -0.24228771\n",
      "0.9433239 0.9445358\n",
      "0.8512943 0.74122924\n",
      "0.71211946 -0.47234163\n",
      "0.92187077 0.3230556\n",
      "0.33666176 0.951184\n",
      "0.63287663 0.73887765\n",
      "0.6627183 0.02276632\n",
      "0.84206235 0.88983256\n",
      "0.5964403 -0.14841777\n",
      "0.72104937 -0.28160846\n",
      "0.87898505 0.47393405\n",
      "0.5877734 0.34475556\n",
      "0.1398241 0.15781023\n",
      "0.409217 -0.24575369\n",
      "0.95118743 0.9482317\n",
      "0.92831933 0.8434562\n",
      "0.42171037 0.5308257\n",
      "0.8133704 -0.764122\n",
      "0.9107938 0.9558863\n",
      "0.40559906 0.5298197\n",
      "0.88266367 0.9596432\n",
      "0.35718566 -0.2769009\n",
      "0.49028903 -0.43118173\n",
      "0.839424 -0.13644047\n",
      "0.58343834 0.2416626\n",
      "0.74240077 -0.017645499\n",
      "0.9784381 -0.3978986\n",
      "0.79349124 0.4294385\n",
      "0.34488434 -0.5587601\n",
      "0.33872917 -0.11750183\n",
      "0.9657355 0.76769245\n",
      "0.8422564 -0.23359393\n",
      "0.91949314 0.77228665\n",
      "0.63046587 0.68190044\n",
      "0.9865909 -0.11897389\n",
      "0.4847786 -0.6791088\n",
      "0.21668336 0.4798109\n",
      "0.89933413 -0.33021593\n",
      "0.90803283 -0.32098186\n",
      "0.9602118 -0.24397077\n",
      "0.48395568 0.8556936\n",
      "0.7266198 0.89548796\n",
      "0.24033049 -0.4395921\n",
      "0.8766436 -0.5142073\n",
      "0.5406185 -0.82816887\n",
      "0.66895014 0.54561716\n",
      "0.9563658 0.46299326\n",
      "306 94\n"
     ]
    }
   ],
   "source": [
    "def calc_euclidean(feat1, feat2):\n",
    "    feat1 = feat1.ravel()\n",
    "    feat2 = feat2.ravel()\n",
    "    return torch.dot(feat1, feat2) / (torch.linalg.norm(feat1) * torch.linalg.norm(feat2))\n",
    "ok = 0\n",
    "fales = 0\n",
    "for i in range(200):\n",
    "    a, p, n = val_dataset.__getitem__(i)\n",
    "    a = torch.from_numpy(a).float()\n",
    "    p = torch.from_numpy(p).float()\n",
    "    n = torch.from_numpy(n).float()\n",
    "\n",
    "    feat_a = model(a.unsqueeze(0))\n",
    "    feat_p = model(p.unsqueeze(0))\n",
    "    feat_n = model(n.unsqueeze(0))\n",
    "\n",
    "    d_ap = calc_euclidean(feat_a, feat_p).detach().cpu().numpy()\n",
    "    d_an = calc_euclidean(feat_a, feat_n).detach().cpu().numpy()\n",
    "    print(d_ap, d_an)\n",
    "    if d_ap>0.5:\n",
    "        ok += 1\n",
    "    else:\n",
    "        fales += 1\n",
    "    if d_an<=0.5:\n",
    "        ok += 1\n",
    "    else:\n",
    "        fales += 1\n",
    "\n",
    "print(ok, fales)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}